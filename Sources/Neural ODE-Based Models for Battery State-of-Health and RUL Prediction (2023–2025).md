# Neural ODE-Based Models for Battery State-of-Health and RUL Prediction (2023‚Äì2025)

## Introduction

Battery State-of-Health (SOH) and Remaining Useful Life (RUL) prediction are critical for reliable battery management in electric vehicles and energy storage systems. Traditional data-driven models (e.g. LSTM, GRU) have shown promise but often struggle with capturing complex degradation dynamics and long-term dependencies. **Neural Ordinary Differential Equations (NODEs)** have emerged as a powerful framework to model battery degradation as a continuous-time process, offering a principled way to incorporate temporal dynamics[\[1\]](file://file_000000009f6c6246adbcf605606da613#:~:text=with%20neural%20ODEs%2C%20augmented%20neural,recurrent%20ODEs%20are%20compared%2021). Recent research (2023‚Äì2025) has focused on **hybrid NODE-based models** that integrate NODEs with advanced architectures ‚Äì such as attention mechanisms, convolutional and recurrent networks, graph neural networks, and physics-informed components ‚Äì to improve prediction accuracy, generalizability, and interpretability. This report presents a detailed literature review of these NODE-based models for battery prognostics, comparing their architectures, input features, performance, and interpretability. Key trends, challenges, and future directions (e.g. toward universal battery models and physically constrained networks) are also discussed.

## Recent NODE-Based Models for Battery SOH and RUL

Below we summarize several state-of-the-art models that leverage Neural ODEs for battery life prediction. Each model introduces unique architectural innovations by combining NODEs with other deep learning components or domain knowledge.

### Augmented Neural ODE (ANODE)

**Architecture & Innovation:** *Augmented Neural ODE (ANODE)* extends the original Neural ODE by adding extra latent dimensions to the ODE‚Äôs state space[\[2\]](file://file_000000009f6c6246adbcf605606da613#:~:text=ANODEs%20have%20been%20introduced%20recently,ODEs.%20In%20161). By introducing an augmented variable **ùíÇ** into the state, ANODE addresses limitations of standard NODEs in representing certain dynamics (e.g. handling non-monotonic or topologically complex mappings with crossing trajectories)[\[2\]](file://file_000000009f6c6246adbcf605606da613#:~:text=ANODEs%20have%20been%20introduced%20recently,ODEs.%20In%20161). In practice, ANODE learns an ODE in an expanded space, which can improve model expressiveness and reduce training loss compared to basic NODEs[\[3\]](file://file_000000009f6c6246adbcf605606da613#:~:text=augmented%20neural%20ODE%20%28ANODE%29%20,shown%20to%20be%20more%2081)[\[4\]](file://file_000000009f6c6246adbcf605606da613#:~:text=robust%20and%20lightweight%20and%20better,ODE%20%5B52%5D.%2082).

**Input Features:** In recent battery studies, ANODE models have been applied using features that capture both capacity fade and charge curve behavior. For example, one approach represents each cycle *k* by the capacity-derived SOH and a sequence of normalized charge times at selected voltage levels[\[5\]](file://file_000000009f6c6246adbcf605606da613#:~:text=have%20shown%20that%20charge%20voltage,were%20also%20included%20in%20105)[\[6\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%92%9A%F0%9D%91%98%20%3D%20). This yields a feature vector *y\<sub\>k\</sub\>* that includes the SOH and a sampled charging voltage profile, effectively encoding the degradation state of the battery at cycle *k*[\[5\]](file://file_000000009f6c6246adbcf605606da613#:~:text=have%20shown%20that%20charge%20voltage,were%20also%20included%20in%20105)[\[6\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%92%9A%F0%9D%91%98%20%3D%20).

**Performance & Dataset:** ANODE has demonstrated high accuracy on standard Lithium-ion battery datasets (e.g. Oxford Battery Degradation Dataset and NASA Randomized Benchmark). In one study, ANODE achieved SOH estimation errors around **1%** and EOL (end-of-life) prediction errors around **5%** when forecasting the last 30% of the battery‚Äôs life[\[1\]](file://file_000000009f6c6246adbcf605606da613#:~:text=with%20neural%20ODEs%2C%20augmented%20neural,recurrent%20ODEs%20are%20compared%2021). It consistently outperformed traditional RNNs (LSTM/GRU) in long-horizon predictions[\[7\]](file://file_000000009f6c6246adbcf605606da613#:~:text=87)[\[8\]](file://file_000000009f6c6246adbcf605606da613#:~:text=retrieving%20underlying%20dynamics%20and%20make,RNN%20can%2088). For instance, across several cells from the Oxford dataset, ANODE kept average SOH RMSE under \~3% for most prediction windows (50‚Äì90% of life used for training)[\[9\]](file://file_000000009f6c6246adbcf605606da613#:~:text=achieves%20the%20best%20performance%2C%20except,an%20average%20RMSEsSOH%20lower%20226)[\[10\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%91%81TP%2FEOL%20%3D70,256). This indicates ANODE‚Äôs strength in capturing battery aging trajectories with limited data. However, some studies noted that ANODE can struggle in scenarios with very abrupt capacity drop-offs (e.g. at certain mid-life ‚Äúknee points‚Äù); at a 60% training horizon, ANODE failed to reproduce a sudden SOH downturn, yielding higher RMSE (\>10%) in those cases[\[11\]](file://file_000000009f6c6246adbcf605606da613#:~:text=At%20%F0%9D%91%81TP%2FEOL%20%3D60,trend%20typical%20of%20battery%20259).

**Computational Efficiency & Interpretability:** Augmenting the state adds parameters but allows a simpler ODE function to fit complex behavior. In practice ANODE training is moderately heavy due to ODE solver integration, but still manageable ‚Äì one study used an adaptive step ODE solver (Dormand‚ÄìPrince) with the adjoint method for gradients[\[12\]](file://file_000000009f6c6246adbcf605606da613#:~:text=In%20neural,Dopri5%2C%20190)[\[13\]](file://file_000000009f6c6246adbcf605606da613#:~:text=dropout%20%28p%3D0,the%20model%20architectures%20is%20194). ANODE‚Äôs predictions are *continuous in time*, which improves smoothness and allows querying SOH at arbitrary timepoints. Interpretability of standard ANODE is still limited (it acts as a black-box neural network inside an ODE), but its improved fit can reduce extrapolation errors. Some recent works treat ANODE as a building block and add other interpretable components (attention layers, physical constraints) to enhance insight, as discussed below.

### Predictor-Corrector RNN (PC-RNN)

**Architecture & Innovation:** *Predictor-Corrector RNN (PC-RNN)* is a novel recurrent architecture that discretizes the Neural ODE solution using a predictor-corrector scheme[\[14\]](file://file_000000009f6c6246adbcf605606da613#:~:text=The%20,RNN%29%2C%2083)[\[8\]](file://file_000000009f6c6246adbcf605606da613#:~:text=retrieving%20underlying%20dynamics%20and%20make,RNN%20can%2088). Essentially, PC-RNN treats the battery degradation as a sequence and applies an explicit one-step ODE integration at each cycle: it first performs a forward Euler **prediction** of the next state, then applies a **correction** using the derivative at the predicted point (akin to a trapezoidal rule)[\[15\]](file://file_000000009f6c6246adbcf605606da613#:~:text=differencing%20is%20used%20to%20obtain,from%20%F0%9D%92%9A%F0%9D%91%98%2C%20namely%2C%20%20171)[\[16\]](file://file_000000009f6c6246adbcf605606da613#:~:text=obtained%20using%20173). This two-step update (predict \+ correct) is encapsulated in an RNN cell, where the ODE‚Äôs derivative function *F(y; Œ∏)* is parameterized by a neural network[\[17\]](file://file_000000009f6c6246adbcf605606da613#:~:text=RNNs%20are%20attractive%20given%20their,53%2C%20168)[\[15\]](file://file_000000009f6c6246adbcf605606da613#:~:text=differencing%20is%20used%20to%20obtain,from%20%F0%9D%92%9A%F0%9D%91%98%2C%20namely%2C%20%20171). The innovation is combining continuous ODE integration with RNN recurrence to improve one-step prediction accuracy and stability.

**Input Features:** PC-RNN uses the same input state representation *y\<sub\>k\</sub\>* as the Neural ODE/ANODE models for fairness. In the reference implementation, the input at each time-step includes the current cycle‚Äôs SOH and sampled charge curve features[\[5\]](file://file_000000009f6c6246adbcf605606da613#:~:text=have%20shown%20that%20charge%20voltage,were%20also%20included%20in%20105)[\[6\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%92%9A%F0%9D%91%98%20%3D%20). These features are fed into the neural network representing *F*, which then produces the derivative for the Euler step. No additional exogenous inputs are used; PC-RNN relies on the sequential structure to capture temporal patterns.

**Performance & Dataset:** In evaluations on the Oxford and NASA datasets, PC-RNN achieved performance comparable to ANODE. It was noted for **accurate RUL forecasting with fewer data** ‚Äì the predictor-corrector mechanism helped leverage small training portions to predict far ahead[\[7\]](file://file_000000009f6c6246adbcf605606da613#:~:text=87)[\[8\]](file://file_000000009f6c6246adbcf605606da613#:~:text=retrieving%20underlying%20dynamics%20and%20make,RNN%20can%2088). For example, when using only 50% of the cycles for training, PC-RNN‚Äôs SOH prediction error remained low (often in the 2‚Äì5% range) and it could more precisely estimate EOL than standard RNNs[\[10\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%91%81TP%2FEOL%20%3D70,256)[\[18\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%28ANODE%29%2C%203.27%25%20%28PC,The%20larger%20errors%20can%20be). In one case (NASA dataset, 70% training), PC-RNN had an average SOH RMSE of \~3.27% versus 4.70% (ANODE) and \>6% (LSTM)[\[10\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%91%81TP%2FEOL%20%3D70,256). This indicates the predictor-corrector scheme yields a slight accuracy edge, likely by reducing integration error each step.

**Computational Efficiency & Interpretability:** PC-RNN forgoes a continuous ODE solver in favor of a fixed Euler integration per time-step, which makes it computationally efficient (comparable to a small RNN) and easier to train than continuous NODEs[\[12\]](file://file_000000009f6c6246adbcf605606da613#:~:text=In%20neural,Dopri5%2C%20190)[\[13\]](file://file_000000009f6c6246adbcf605606da613#:~:text=dropout%20%28p%3D0,the%20model%20architectures%20is%20194). Its architecture explicitly computes *F(y)* at two points (current and predicted next state), offering some interpretability: the correction term (difference between predicted and corrected state) can be analyzed to understand how much the one-step prediction deviates, potentially relating to nonlinear degradation effects. However, like other deep models, PC-RNN‚Äôs internal neural function is a black box. It does have fewer parameters than an equivalent LSTM (e.g. one study found a shallow PC-RNN with 1‚Äì2 layers of 40‚Äì50 neurons was optimal[\[19\]](file://file_000000009f6c6246adbcf605606da613#:~:text=augmented%20200)[\[20\]](file://file_000000009f6c6246adbcf605606da613#:~:text=space%20dimensions%20,RNN%20underlying%20201)). Overall, PC-RNN strikes a balance between ODE-based rigor and RNN simplicity, making it attractive for on-line use in battery management systems.

### GAT-KAN ODE Model (Graph \+ Kolmogorov‚ÄìArnold Network)

**Architecture & Key Innovations:** To exploit rich interdependences in battery data and improve interpretability, Wang *et al.* (2025) proposed a hybrid model combining a **Graph Attention Network (GAT)** with a **Kolmogorov‚ÄìArnold Network (KAN)** based Neural ODE[\[21\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,state%20trajectories%20during%20the%20degradation)[\[22\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=layers%20in%20FC%20,proposed%20to%20replace%20fixed%20activation). The architecture consists of two stages: First, a GAT encodes *spatial/temporal relations* among multiple external health indicators of the battery. Various external parameters (e.g. voltage, temperature, internal resistance, coulombic efficiency, etc.) measured over a recent time window are represented as nodes in a graph, allowing the GAT to learn implicit topological relations (i.e. which indicators and past times are most relevant to degradation)[\[23\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Fig,external%20indicators%20at%20different%20moments)[\[24\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,supplement%20the%20interdependencies%20between%20physical). This yields an enriched feature representation that captures non-linear correlations among indicators without manual feature engineering[\[21\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,state%20trajectories%20during%20the%20degradation). Next, this representation is passed into a **neuronal ODE** whose dynamics are parameterized by a KAN. The KAN is a novel neural network architecture inspired by the Kolmogorov‚ÄìArnold representation theorem, which states any multivariate continuous function can be decomposed into a combination of univariate functions and additions[\[25\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=fully%20connected%20neural%20network%2C%20KAN,the%20binary%20operation%20of%20addition)[\[26\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=These%20matrices%20illustrate%20a%20Kolmogorov%E2%80%93Arnold,5%2C%20which%20is%20constructed%20as). In the KAN, instead of traditional fixed activation functions at neurons, **learnable univariate functions** on each weight are used[\[22\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=layers%20in%20FC%20,proposed%20to%20replace%20fixed%20activation)[\[27\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=functions%20inspired%20by%20the%20Kolmogorov%E2%80%93Arnold,connected%20neural). This allows it to approximate complex non-linear functions more efficiently and with a smaller network size than a fully-connected network[\[27\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=functions%20inspired%20by%20the%20Kolmogorov%E2%80%93Arnold,connected%20neural)[\[25\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=fully%20connected%20neural%20network%2C%20KAN,the%20binary%20operation%20of%20addition). Essentially, KAN serves as a compact yet expressive **ODE function block** within the Neural ODE, mapping the GAT-derived state to its time-derivative. The overall model thus captures *spatial relations* (via GAT) and *continuous temporal dynamics* (via KAN-based ODE). The KAN architecture also introduces residual-like links and structured layers which improve training stability for the ODE and lend interpretability to the learned dynamics[\[26\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=These%20matrices%20illustrate%20a%20Kolmogorov%E2%80%93Arnold,5%2C%20which%20is%20constructed%20as)[\[28\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=approximated%20by%20Kolmogorov%E2%80%93Arnold%20network%20,dynamic%20degradation%20information%20of%20SSLIBs).

**Input Features:** This model is applied to **solid-state Li-ion batteries (SSLIBs)**, which have multiple measurable health indicators. Input features include a set of external sensor and performance measurements ‚Äì e.g. voltage, impedance-related metrics, coulombic efficiency, etc. ‚Äì collected over the last few cycles. These are organized in a graph where *each node represents an indicator at a certain time (cycle)*, and edges encode relationships (such as temporal adjacency or statistical correlations)[\[23\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Fig,external%20indicators%20at%20different%20moments). The GAT learns attention weights for each edge, effectively discovering which past indicator values influence the current state. The output of the GAT is a latent state vector summarizing the ‚Äúimplicit degradation information‚Äù up to the current cycle[\[29\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=obtaining%20comprehensive%20implicit%20information%20during,KAN)[\[21\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,state%20trajectories%20during%20the%20degradation). This latent state is then the initial condition for the KAN-ODE, which evolves it forward in continuous time to predict the future degradation trajectory and ultimately the RUL. Notably, the approach avoids manual feature selection; all relevant signals are fed in, and the GAT+KAN pipeline filters and uses them appropriately.

**Performance & Dataset:** The GAT-KAN ODE model was evaluated on real SSLIB degradation datasets, showing state-of-the-art accuracy. For instance, with only 30% of each battery‚Äôs life used for training, the model achieved an RMSE of **\~16.08 cycles** in RUL prediction, outperforming baseline deep networks like stacked CNNs, LSTMs, GRUs, and a CNN-LSTM hybrid (which had RMSE in the 17‚Äì20 range under the same conditions)[\[30\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Comparison%20results%20of%20different%20methods,0726). With more training data (50% and 80% of life), the advantages became even more pronounced: at 50% training, the model‚Äôs RUL RMSE was about **10.05** cycles vs. 11‚Äì17 cycles for others[\[31\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Method%20%F0%9D%91%852%20MAE%20SMAPE%20RMSE,9132); at 80% training, it attained **4.25** cycles RMSE, on par with or better than all benchmarks[\[32\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Table%203%20Comparison%20results%20of,5371). In terms of R¬≤, it consistently scored above 0.95 even with limited data[\[30\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Comparison%20results%20of%20different%20methods,0726)[\[31\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Method%20%F0%9D%91%852%20MAE%20SMAPE%20RMSE,9132). Importantly, this approach also greatly reduced model complexity. Thanks to the efficient function approximation by KAN, the total trainable parameters were reduced by \~39‚Äì49% compared to conventional fully-connected architectures[\[33\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=optimal%20minimum%20root%20mean%20square,provide%20new%20perspectives%20and%20solutions), without sacrificing accuracy. Such parameter reduction is significant for deployment on resource-constrained systems.

**Computational Efficiency & Interpretability:** By replacing large fully-connected layers with the Kolmogorov‚ÄìArnold network, the model is lighter and faster ‚Äì the authors report nearly *half the number of parameters* vs comparable models[\[33\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=optimal%20minimum%20root%20mean%20square,provide%20new%20perspectives%20and%20solutions). The continuous ODE formulation means the model outputs a whole trajectory of SOH decline, enabling flexible RUL estimation (e.g. one can solve the ODE until SOH drops to 80% to get RUL). The inclusion of KAN offers interpretability: since KAN is built on univariate function compositions, one can, in principle, inspect the learned univariate functions to understand how each input contributes to degradation[\[27\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=functions%20inspired%20by%20the%20Kolmogorov%E2%80%93Arnold,connected%20neural). Moreover, using GAT provides insight into feature importance ‚Äì the attention weights indicate which sensor signals or operating conditions most strongly influence the model‚Äôs RUL predictions. This addresses a common criticism of deep learning models (their ‚Äúblack-box‚Äù nature) by providing some **transparency** in how predictions are made. Overall, the GAT-KAN ODE sets a new benchmark by achieving high accuracy with fewer parameters and offering interpretable components, making it a notable step toward practical and explainable battery prognostics[\[34\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=ordinary%20differential%20equation%20,in%20the%20real%20SSLIBs%20degradation)[\[28\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=approximated%20by%20Kolmogorov%E2%80%93Arnold%20network%20,dynamic%20degradation%20information%20of%20SSLIBs).

### ACLA: Attention‚ÄìCNN‚ÄìLSTM ANODE Model

**Architecture & Key Innovations:** *ACLA* (proposed by Li *et al.* 2025\) is a hybrid framework that embeds an **Augmented Neural ODE** within a deep sequence model composed of **attention**, **CNN**, and **LSTM** modules[\[35\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=We%20construct%20a%20hybrid%20model,which%20integrates%20the%20attention%20mechanism). The name ‚ÄúACLA‚Äù reflects its components: **A**ttention \+ **C**NN \+ **L**STM within an **A**NODE framework. The model architecture (see Fig. 3 in the paper) operates as follows[\[36\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3)[\[37\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=Fig,the%20model%20architecture): First, an **attention layer** processes the input features at each cycle to adaptively weight the most informative elements (e.g. certain portions of the charging curve or particular features get higher attention scores)[\[38\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3)[\[39\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model). This helps the model focus on crucial indicators of degradation at each time step. Next, the attended features pass through two **convolutional neural network (CNN)** layers, which perform local feature extraction[\[40\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=CNNs%20are%20commonly%20used%20for,They). The CNN is adept at capturing patterns in the charging voltage-time sequence (for example, identifying shape characteristics of the charge curve that correlate with aging)[\[40\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=CNNs%20are%20commonly%20used%20for,They). The CNN outputs are then fed into an **LSTM** network, which captures the temporal dependencies across cycles (the long-term evolution of those features)[\[35\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=We%20construct%20a%20hybrid%20model,which%20integrates%20the%20attention%20mechanism)[\[36\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3). This LSTM essentially serves as a high-level summarizer of the battery‚Äôs degradation history. Finally, the LSTM‚Äôs hidden state is passed through a fully-connected layer into an **Augmented Neural ODE solver**[\[36\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3)[\[41\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=connected%20layer%20,data%20to%20the%20ANODE%20solver). The ANODE module treats the LSTM-derived state as an initial condition and integrates a learned ODE to predict the future SOH trajectory continuously. In training, the ANODE component backpropagates through the ODE solver (using the adjoint method) and learns the dynamics of SOH decay. By integrating attention, CNN, and LSTM, ACLA can capture **complex temporal dynamics** and multi-scale features of battery aging more effectively than a plain NODE[\[42\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=match%20at%20L268%20on%20NODE,The%20proposed%20model%20integrates)[\[43\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=on%20NODE%20in%20combination%20with,The%20proposed%20model%20integrates). It essentially augments the NODE with powerful sequence feature learners.

**Input Features:** ACLA takes as input the *constant-current charging phase data* of each cycle. Specifically, it uses the **normalized charging time at specific voltage points** (much like the earlier models) along with the current cycle‚Äôs SOH[\[44\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model%20employs%20normalized%20charging%20time,to%20specific%20voltages%20in%20the)[\[45\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=,Specifically%2C%20the%20input%20feature). In practice, the input at cycle *k* is a vector comprising the SOH\<sub\>k\</sub\> plus a fixed-length sequence of timestamps corresponding to pre-defined voltage levels during CC charging[\[45\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=,Specifically%2C%20the%20input%20feature). This is equivalent to the feature vector used in the Oxford/NASA datasets (21 or 19 time features \+ 1 SOH)[\[5\]](file://file_000000009f6c6246adbcf605606da613#:~:text=have%20shown%20that%20charge%20voltage,were%20also%20included%20in%20105)[\[6\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%92%9A%F0%9D%91%98%20%3D%20). These features are fed into the attention layer which emphasizes certain voltage-time points more than others if they signal rapid degradation (for example, an increase in time to reach a high voltage might indicate growing internal resistance, and the attention mechanism can learn to highlight that). Notably, ACLA was trained on a combination of the NASA and Oxford datasets and then **validated on two different datasets (TJU and HUST)** to test its generalization[\[46\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=remaining). This means the input feature formulation was general enough to apply across different battery datasets and even different cathode chemistries (Oxford cells were LCO/NMC, while TJU/HUST might include LFP or NCM cells).

**Performance & Dataset:** ACLA demonstrated excellent accuracy and generalizability. Trained on one set of batteries and tested on entirely separate datasets, it still yielded low error rates. In cross-dataset validation, ACLA‚Äôs SOH estimation achieved **RMSE as low as 1.01%** on the Tianjin University (TJU) dataset and **2.24%** on the HUST dataset[\[47\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=useful%20of%20life,datasets%20and%20validated%20on%20the). These are remarkably low errors, indicating minimal deviation from true capacity values. Moreover, ACLA simultaneously predicts RUL (or EOL): in the Oxford dataset, for example, it achieved a RUL prediction error of only \~5.8 cycles on a test battery (B6)[\[48\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=adeptly%20applied%20to%20the%20prediction,RUL%2C%20achieving%20high%20accuracy%20results). Compared to baseline models (standard NODE and ANODE without the attention/CNN/LSTM enhancements), ACLA was significantly more accurate[\[49\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=useful%20of%20life,datasets%20and%20validated%20on%20the)[\[50\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=ordinary%20differential%20equation%20%28ODE%29,ODE%20%28NODE%29%20%5B35%5D%20and). The inclusion of the attention mechanism was found to particularly improve performance ‚Äì an ablation study showed that removing the attention layer led to worse accuracy, confirming that focusing on salient features is beneficial[\[43\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=on%20NODE%20in%20combination%20with,The%20proposed%20model%20integrates)[\[51\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=complex%20temporal%20dynamics,the%20model%E2%80%99s%20performance%20with%20its). ACLA‚Äôs strong results on *unseen* datasets suggest it has better generalization than many previous models, addressing the common issue that models trained on one dataset often fail on others.

**Computational Efficiency & Interpretability:** The trade-off for ACLA‚Äôs accuracy is a more complex model (multiple layers of CNN, LSTM, plus ODE solver). Training can be computationally intensive, as it involves backpropagating through an ODE integration for each epoch and handling sequences with CNN/LSTM. The authors report using an augmented dimension of 20 for the ANODE and a Dopri5 ODE solver, with a batch size of 1 (sequential training)[\[52\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3.2%20Implementation%20and%20hyper)[\[53\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=The%20ANODE%20model%20uses%20an,20%20and%20applies%20the%20default). This indicates training was likely slow but feasible on sequence data. At inference time, the model can output both the current SOH and forecast the RUL (e.g. by integrating until SOH=80% threshold). Interpretability is partly addressed by the *attention weights*: one can inspect which features (voltage segments) the model is focusing on at each prediction, providing insight into the degradation indicators[\[38\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3)[\[39\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model). Additionally, by integrating an LSTM, the model retains some explainability in terms of known time-series patterns (for instance, the LSTM state could be interpreted as a learned health indicator aggregating past behavior). Nonetheless, the presence of CNN and LSTM means ACLA is still largely a black-box. The authors‚Äô effort to validate on multiple datasets does, however, inspire confidence that the model is not just overfitting peculiarities ‚Äì it‚Äôs extracting generalizable degradation features. In summary, ACLA represents a fusion of deep learning techniques within a Neural ODE scaffold to yield both high accuracy and the ability to predict continuous SOH/RUL trajectories across different battery populations.

### Other Emerging Hybrid NODE Approaches (2023‚Äì2025)

Beyond the specific models above, several other recent works have explored innovative ways to integrate NODEs with disparate architectures or domain knowledge for battery prognostics:

* **Physics-Informed Neural ODEs:** A *Scientific Machine Learning (SciML)* approach by Dandekar & Panat (2024) combines Neural ODEs with **Universal Differential Equations (UDEs)** to embed physical battery degradation knowledge into the model[\[54\]](https://arxiv.org/html/2410.14347v1#:~:text=to%20the%20prediction%20and%20long,that%20both%20the%20predictions%20and)[\[55\]](https://arxiv.org/html/2410.14347v1#:~:text=forecasts%20reflect%20practical%20conditions,This%20integration%20of%20data). In this framework, known equations (e.g. empirical aging laws or diffusion dynamics) are incorporated alongside neural network components in the ODE, creating a hybrid that captures both known physics and unknown effects[\[56\]](https://arxiv.org/html/2410.14347v1#:~:text=Differential%20Equations%20,achieved%20a%20mean%20squared%20error)[\[57\]](https://arxiv.org/html/2410.14347v1#:~:text=systems%2C%20SciML%20integrates%20domain%20knowledge,49%20in%20the%20NeuralODE). This approach yielded improved prediction accuracy with greater transparency ‚Äì for instance, the SciML model achieved lower error (MSE ‚âà 9.90) with a UDE-enhanced Neural ODE, compared to MSE ‚âà 11.55 for a pure Neural ODE on the same data[\[55\]](https://arxiv.org/html/2410.14347v1#:~:text=forecasts%20reflect%20practical%20conditions,This%20integration%20of%20data). By integrating domain knowledge, the model required less data to train and provided physically plausible predictions (e.g. obeying expected aging trends)[\[58\]](https://arxiv.org/html/2410.14347v1#:~:text=term%20battery%20health%20and%20forecasting,strengths%20in%20interpretability%20and%20scalability)[\[59\]](https://arxiv.org/html/2410.14347v1#:~:text=predictive%20accuracy%20while%20reducing%20data,to%20the%20sustainability%20of%20energy). Such physics-informed NODEs address the lack of generalizability in purely data-driven models and represent a step toward **universal battery models** that can adapt across chemistries and usage profiles.

* **Transformer-Based Architectures with Physical Constraints:** While transformers are not yet commonly combined directly with Neural ODEs in literature, researchers are leveraging transformers for battery life prediction in parallel work. For example, Hou *et al.* (2025) proposed an *enhanced Transformer model with physical information constraints* for joint SOH and RUL prediction[\[60\]](https://www.researchgate.net/publication/395488447_Joint_prediction_of_SOH_and_RUL_for_Lithium-ion_batteries_by_an_enhanced_Transformer_model_with_physical_information_constraints#:~:text=Joint%20prediction%20of%20SOH%20and,model%20with%20physical%20information%20constraints). Their model uses the self-attention mechanism to capture long-range dependencies in cycling data and enforces physical consistency (such as monotonic capacity decline or known empirical relations) during training. This resulted in high accuracy (reported RMSE of 0.0136 for SOH and 5.80 cycles for RUL on a test cell)[\[48\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=adeptly%20applied%20to%20the%20prediction,RUL%2C%20achieving%20high%20accuracy%20results). Although this approach does not use a continuous ODE solver, it exemplifies the trend of **injecting physical knowledge into deep models**. It is reasonable to anticipate future works that unite transformer encoders with Neural ODE decoders ‚Äì e.g. using a transformer to process irregular usage data into an initial state, which a Neural ODE then evolves in time ‚Äì to harness the strengths of both frameworks.

* **Graph Neural ODEs:** Another theoretical direction is applying Neural ODE concepts on graph-structured battery models. The GAT-KAN model discussed earlier is one instance, but more generally, battery systems (especially packs of cells) can be seen as graphs. A continuous-depth GNN (Graph NODE) could model spatial-temporal interactions in a battery pack or across electrode sections. While we have not seen a dedicated ‚ÄúGraph Neural ODE‚Äù for battery packs in 2023‚Äì2025 literature, the pieces (graph networks for spatial relations[\[61\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=work%20,batteries%20to%20improve%20the%20computational) and NODEs for time dynamics) are clearly being assembled, as in the GAT-KAN work. This could lead to **pack-level RUL predictions** where cell-to-cell variations and thermal gradients are captured by a neural ODE evolving over a graph of cells or components.

* **Uncertainty Quantification in NODE Models:** A notable challenge for battery prognostics is knowing the confidence of a prediction. Emerging research in other domains on *Bayesian Neural ODEs* and *Neural Controlled Differential Equations* (NCDEs) can be applied to batteries to quantify prediction uncertainty and handle irregular time-series data. For instance, a Bayesian Neural ODE can provide probabilistic SOH forecasts, indicating a range of possible RULs with confidence intervals. While specific battery-focused studies are sparse, this is a promising avenue to make NODE models more robust and trustworthy in safety-critical applications.

The proliferation of these hybrid approaches underscores an overarching trend: **integrative models** that blend deep learning with physical modeling or combine multiple network types are leading the way in battery SOH/RUL estimation. Table 1 provides a comparative overview of the key models discussed, highlighting their architecture, inputs, performance, and other attributes.

## Comparative Analysis of NODE-Based Battery Models

To facilitate comparison, **Table 1** summarizes the main characteristics of each model reviewed above, along several dimensions: architecture & innovations, input features, performance (typical accuracy and datasets), and notes on efficiency/interpretability.

| Model | Architecture & Key Innovations | Input Features | Performance (Accuracy & Dataset) | Efficiency & Interpretability |
| :---- | :---- | :---- | :---- | :---- |
| **Neural ODE** (baseline) | ODE defined by a neural network; continuous-time modeling of battery SOH. (Chen et al. 2018 framework) | Cycle capacity (SOH) and sampled charge voltage-time points[\[5\]](file://file_000000009f6c6246adbcf605606da613#:~:text=have%20shown%20that%20charge%20voltage,were%20also%20included%20in%20105)[\[6\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%92%9A%F0%9D%91%98%20%3D%20). | \~1‚Äì5% SOH error when predicting last 30% of life[\[1\]](file://file_000000009f6c6246adbcf605606da613#:~:text=with%20neural%20ODEs%2C%20augmented%20neural,recurrent%20ODEs%20are%20compared%2021). Tested on Oxford & NASA datasets; outperforms LSTM/GRU at long-horizon prediction[\[7\]](file://file_000000009f6c6246adbcf605606da613#:~:text=87)[\[8\]](file://file_000000009f6c6246adbcf605606da613#:~:text=retrieving%20underlying%20dynamics%20and%20make,RNN%20can%2088). | Needs ODE solver (moderate compute). Smooth interpolation of SOH over time; black-box dynamics, limited interpretability. |
| **ANODE** (Augmented NODE) | Neural ODE with augmented latent dimensions to improve expressiveness[\[2\]](file://file_000000009f6c6246adbcf605606da613#:~:text=ANODEs%20have%20been%20introduced%20recently,ODEs.%20In%20161). Handles complex dynamics (e.g. non-monotonic trends) better than plain NODE. | Same as Neural ODE (SOH \+ charge curve features). | Achieved \<3% SOH RMSE on average (Oxford data) for 50‚Äì90% training range[\[9\]](file://file_000000009f6c6246adbcf605606da613#:~:text=achieves%20the%20best%20performance%2C%20except,an%20average%20RMSEsSOH%20lower%20226). \~1% SOH and 5% RUL error at 30% training[\[1\]](file://file_000000009f6c6246adbcf605606da613#:~:text=with%20neural%20ODEs%2C%20augmented%20neural,recurrent%20ODEs%20are%20compared%2021). More robust and lower loss than NODE[\[3\]](file://file_000000009f6c6246adbcf605606da613#:~:text=augmented%20neural%20ODE%20%28ANODE%29%20,shown%20to%20be%20more%2081)[\[4\]](file://file_000000009f6c6246adbcf605606da613#:~:text=robust%20and%20lightweight%20and%20better,ODE%20%5B52%5D.%2082). | Training is somewhat heavier (augmented state adds parameters). Still black-box, though augmented dimensions can help fit without overfitting. |
| **PC-RNN** (Predictor-Corrector RNN) | Discrete ODE integration as an RNN: forward Euler predictor \+ trapezoidal corrector per step[\[15\]](file://file_000000009f6c6246adbcf605606da613#:~:text=differencing%20is%20used%20to%20obtain,from%20%F0%9D%92%9A%F0%9D%91%98%2C%20namely%2C%20%20171)[\[16\]](file://file_000000009f6c6246adbcf605606da613#:~:text=obtained%20using%20173). Learns dynamics via recursive updates. | Same features per cycle (SOH and curve points). | Matches ANODE accuracy, often \~2‚Äì4% SOH RMSE with limited data[\[10\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%91%81TP%2FEOL%20%3D70,256). At 70% training (NASA), PC-RNN RMSE ‚âà3.3% vs. 4.7% (ANODE) and \>6% (LSTM)[\[10\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%91%81TP%2FEOL%20%3D70,256). Excellent RUL forecasting with fewer cycles[\[7\]](file://file_000000009f6c6246adbcf605606da613#:~:text=87)[\[8\]](file://file_000000009f6c6246adbcf605606da613#:~:text=retrieving%20underlying%20dynamics%20and%20make,RNN%20can%2088). | Efficient (no continuous solver; uses simple Euler steps). Fewer parameters than LSTM. Some interpretability from error correction term each step. |
| **KAN-ODE** (GAT \+ KAN) | Graph Attention Network to encode relations among multiple indicators, feeding a Kolmogorov‚ÄìArnold Network ODE[\[21\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,state%20trajectories%20during%20the%20degradation)[\[22\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=layers%20in%20FC%20,proposed%20to%20replace%20fixed%20activation). KAN provides a compact, interpretable ODE function (learnable univariate activations)[\[27\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=functions%20inspired%20by%20the%20Kolmogorov%E2%80%93Arnold,connected%20neural). | Multi-sensor health indicators over recent cycles (voltage, impedance, etc.), treated as graph nodes[\[23\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Fig,external%20indicators%20at%20different%20moments). | **High accuracy on solid-state LIBs**: e.g. RUL RMSE ‚âà16 cycles (30% training) vs 17‚Äì20+ for LSTM/CNN baselines[\[30\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Comparison%20results%20of%20different%20methods,0726); RMSE ‚âà10 cycles (50% training) vs 12‚Äì17 for others[\[31\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Method%20%F0%9D%91%852%20MAE%20SMAPE%20RMSE,9132). R¬≤ up to 0.96[\[62\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Method%20%F0%9D%91%852%20MAE%20SMAPE%20RMSE,9132). \~40‚Äì50% fewer parameters than traditional deep models[\[33\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=optimal%20minimum%20root%20mean%20square,provide%20new%20perspectives%20and%20solutions). | Smaller model size (half as many parameters) improves speed. ODE solver overhead present but offset by lightweight KAN layers. Good interpretability: attention weights highlight important features; KAN‚Äôs structure yields insight into learned degradation function[\[27\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=functions%20inspired%20by%20the%20Kolmogorov%E2%80%93Arnold,connected%20neural)[\[25\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=fully%20connected%20neural%20network%2C%20KAN,the%20binary%20operation%20of%20addition). |
| **ACLA** (Attn‚ÄìCNN‚ÄìLSTM ANODE) | Augmented NODE integrated with attention, CNN, and LSTM modules[\[35\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=We%20construct%20a%20hybrid%20model,which%20integrates%20the%20attention%20mechanism). Attention focuses on important voltage segments; CNN extracts charge curve features; LSTM captures long-term trends; ANODE provides continuous SOH/RUL output. | SOH and normalized charge times at specific voltages (constant-current phase)[\[45\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=,Specifically%2C%20the%20input%20feature). Input sequence per cycle (\~20 features) ‚Üí attention ‚Üí CNN ‚Üí LSTM. | **State-of-the-art accuracy & generalization:** SOH RMSE \~1.0% (TJU dataset) and 2.2% (HUST) when trained on NASA+Oxford[\[47\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=useful%20of%20life,datasets%20and%20validated%20on%20the). On Oxford, RUL error \~5.8 cycles for a test cell[\[48\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=adeptly%20applied%20to%20the%20prediction,RUL%2C%20achieving%20high%20accuracy%20results). Significantly outperforms baseline NODE/ANODE (no attention/CNN/LSTM)[\[50\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=ordinary%20differential%20equation%20%28ODE%29,ODE%20%28NODE%29%20%5B35%5D%20and). | Most computationally complex (sequential CNN/LSTM plus ODE solver). Training on multiple datasets was required. Partially interpretable via attention weights (highlights which features signal degradation)[\[63\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3)[\[39\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model). Continuous output from ANODE (predicts full trajectory). |
| **Physics-informed NODE** (e.g. UDE-NODE hybrid) | Neural ODE augmented with physical equations or constraints (Universal DE)[\[56\]](https://arxiv.org/html/2410.14347v1#:~:text=Differential%20Equations%20,achieved%20a%20mean%20squared%20error). Embeds known degradation kinetics into the model to guide learning. | Typically uses same data as above (capacity, voltage etc.), plus any known physical parameters (if available) for constraints. | Demonstrated on research data (e.g. Sandia labs cells): achieved lower error than pure data-driven models (MSE improved from 11.55 to 9.90 with physics-informed ODE)[\[55\]](https://arxiv.org/html/2410.14347v1#:~:text=forecasts%20reflect%20practical%20conditions,This%20integration%20of%20data). Shows better long-term extrapolation (obeys expected capacity fade trends)[\[58\]](https://arxiv.org/html/2410.14347v1#:~:text=term%20battery%20health%20and%20forecasting,strengths%20in%20interpretability%20and%20scalability). | Efficiency depends on complexity of embedded physics (solving combined equations). Much improved interpretability and generalizability ‚Äì model parameters tie to physical meaning (e.g. SEI growth rates), enabling *universal* applicability across conditions[\[64\]](https://arxiv.org/html/2410.14347v1#:~:text=Wang%20et%20al.%20,with%20Neural%20ODEs%20or%20UDEs)[\[65\]](https://arxiv.org/html/2410.14347v1#:~:text=Ye%20et%20al.%20,world%20validation%20remains%20a%20limitation). |

**Table 1: Comparison of recent NODE-based models for battery SOH/RUL prediction.** ANODE \= Augmented Neural ODE; PC-RNN \= Predictor-Corrector RNN; KAN \= Kolmogorov‚ÄìArnold Network; GAT \= Graph Attention Network; ACLA \= Attention-CNN-LSTM ANODE. Metrics are summarized from key references. RMSE is root-mean-square error (for SOH % or for cycle count in RUL as specified). Lower is better. RUL error units are in cycles.

## State-of-the-Art Trends

Several **trends** can be identified from the above developments:

* **Integration of Diverse Learning Modules:** Recent models are rarely just ‚Äúplain‚Äù Neural ODEs. The tendency is to incorporate **hybrid architectures** that combine the strengths of various networks: for example, attention mechanisms to handle long sequences[\[43\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=on%20NODE%20in%20combination%20with,The%20proposed%20model%20integrates), CNNs to extract informative features from voltage curves[\[40\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=CNNs%20are%20commonly%20used%20for,They), LSTMs or transformers to capture long-term dependencies, and graph neural networks to exploit spatial or cross-sensor relationships[\[21\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,state%20trajectories%20during%20the%20degradation)[\[61\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=work%20,batteries%20to%20improve%20the%20computational). This fusion addresses the multifaceted nature of battery aging, which involves complex interactions and patterns at different scales. As a result, models like ACLA and the GAT-KAN ODE achieve superior accuracy by **jointly learning from multiple feature representations** (temporal sequences, spatial graphs, etc.).

* **Focus on Generalizability and Transferability:** A notable shift in recent works is testing models on *different datasets or operating conditions* than they were trained on. Earlier data-driven models often excelled on one dataset but failed to generalize. New studies explicitly aim for models that can generalize across battery types. For instance, ACLA was validated on cells from totally different sources (TJU, HUST) than the training data[\[46\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=remaining), and it still performed well, indicating improved generalization. Similarly, physics-informed approaches seek **universal patterns** by injecting domain knowledge, making models less reliant on particular data distributions[\[64\]](https://arxiv.org/html/2410.14347v1#:~:text=Wang%20et%20al.%20,with%20Neural%20ODEs%20or%20UDEs)[\[65\]](https://arxiv.org/html/2410.14347v1#:~:text=Ye%20et%20al.%20,world%20validation%20remains%20a%20limitation). The goal of a *‚Äúuniversal battery model‚Äù* ‚Äì one that could be deployed on any battery with minimal re-training ‚Äì is driving research into architectures that combine learning with physical laws or that leverage very large and diverse training datasets.

* **Improved Efficiency and Lightweight Models:** There is increasing awareness of the computational constraints in real-world BMS implementations. Techniques to reduce model complexity are being embraced. The KAN-based approach explicitly aimed to replace large fully-connected networks with a more compact representation, cutting model size nearly in half[\[33\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=optimal%20minimum%20root%20mean%20square,provide%20new%20perspectives%20and%20solutions). Similarly, others have explored **lightweight network designs** (e.g. filtered features, smaller ensembles) as referenced in reviews[\[66\]](https://www.researchgate.net/publication/395488447_Joint_prediction_of_SOH_and_RUL_for_Lithium-ion_batteries_by_an_enhanced_Transformer_model_with_physical_information_constraints#:~:text=demanding%20conditions,techniques%20and%20the%20machine%20learning)[\[67\]](https://www.researchgate.net/publication/395488447_Joint_prediction_of_SOH_and_RUL_for_Lithium-ion_batteries_by_an_enhanced_Transformer_model_with_physical_information_constraints#:~:text=study%20discusses%20recent%20approaches%20to,the%20standard%20in%20SOH%20prediction). The trend is toward models that are not just accurate, but also *deployable* ‚Äì meaning low memory footprint and reasonable inference time on automotive-grade hardware. Neural ODEs can be computationally heavy due to ODE solver calls; thus, research is looking at adaptive solvers or simplified ODE formulations (like PC-RNN‚Äôs Euler step) to speed up computation without losing too much accuracy.

* **Explainability and Physics Integration:** Battery stakeholders often demand interpretability ‚Äì knowing *why* a model predicts a certain RUL is important for trust and insight. We see a trend of making black-box models more **explainable**. KAN provides interpretability by construction (mapping to univariate function contributions)[\[27\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=functions%20inspired%20by%20the%20Kolmogorov%E2%80%93Arnold,connected%20neural). Attention layers in ACLA and transformer models offer a window into which features are deemed important[\[63\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3)[\[39\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model). Moreover, physics-informed models inherently align with known behavior (e.g. the model will not predict an *increase* in capacity since decay processes are embedded), which reassures users that predictions are physically reasonable. This convergence of data-driven and physics-based modeling ‚Äì often termed **‚Äúgray-box‚Äù** modeling ‚Äì is a significant trend aiming to get the best of both worlds: high accuracy from learning and robustness from physical knowledge.

* **Joint Estimation of SOH and RUL:** Rather than treating SOH estimation and RUL prediction as separate tasks, many modern approaches perform *joint learning* of both. For example, the enhanced transformer model and ACLA explicitly output both current SOH and the RUL (or directly the EOL cycle)[\[44\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model%20employs%20normalized%20charging%20time,to%20specific%20voltages%20in%20the)[\[48\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=adeptly%20applied%20to%20the%20prediction,RUL%2C%20achieving%20high%20accuracy%20results). Joint models can share features between tasks and often improve overall prognostic consistency (e.g. ensuring the predicted SOH trajectory leads to the predicted RUL). This multi-task learning trend results in comprehensive health assessment models that provide a full picture of battery status.

In summary, the state-of-the-art is characterized by **hybrid, multi-scale models** that are more accurate, generalizable, and interpretable than their predecessors. These models leverage the continuous-time strength of NODEs while mitigating their weaknesses through clever integrations. As a result, battery life prediction is inching closer to the level of reliability needed for real-world deployment in diverse scenarios.

## Challenges and Limitations

Despite the progress, applying Neural ODE-based models to battery prognostics comes with several challenges and limitations:

* **Data Limitations and Generalization:** High-fidelity battery degradation data is limited, often comprising only tens of cells under specific lab conditions. NODE models, especially complex hybrid ones, can easily overfit these small datasets. Many models still require careful training/validation splits and can struggle when confronted with data from a different source (different manufacturer or usage regime). While models like ACLA and physics-informed NODEs attempt to generalize, truly **universal models** remain elusive. A model trained on one chemistry (e.g. NMC) may not automatically transfer to another (e.g. LFP) without additional tuning, because the underlying degradation patterns differ. This highlights the need for broader datasets or adaptation techniques to cover diverse scenarios.

* **Computational Complexity:** Neural ODE models introduce overhead due to solving differential equations during training and inference. This can lead to long training times and potential numerical stability issues (especially if the ODE is stiff, as battery systems can be near end-of-life). Complex architectures like ACLA, with CNN and LSTM components, further increase computation load. This complexity can be a barrier for on-board Battery Management Systems that have limited processing power. Even though methods like adjoint sensitivity reduce memory usage, training a NODE on long sequences (hundreds of cycles) can be demanding. **Real-time inference** is also a concern ‚Äì solving an ODE for each prediction might be too slow for certain BMS that require instantaneous health estimation. Techniques like PC-RNN partially address this by using fixed-step updates, but there is a trade-off in accuracy. Ensuring that these advanced models can run efficiently on edge devices (or finding ways to distill them into simpler forms) is still a challenge.

* **Model Stability and Extrapolation:** Predicting far into the future (extrapolating beyond the training range) is inherently difficult. Neural ODEs can in theory extrapolate SOH as a smooth curve, but if the learned dynamics are slightly off, the errors accumulate over long horizons. Some studies observed that ANODE or NODE models sometimes failed to predict the rapid nonlinear capacity drop near EOL (the ‚Äúknee point‚Äù) accurately[\[11\]](file://file_000000009f6c6246adbcf605606da613#:~:text=At%20%F0%9D%91%81TP%2FEOL%20%3D60,trend%20typical%20of%20battery%20259). If the ODE doesn‚Äôt capture the accelerated aging phase, it might predict a remaining life that is too optimistic. This is partly due to limited examples of knee behavior in training data and partly due to the model‚Äôs continuity assumptions. In reality, degradation can have discrete events (e.g. sudden loss of active material) that are hard for a continuous model to capture. While augmented states and hybrid networks help, **extrapolation reliability** remains a limitation. Careful validation is needed, and models might need to be re-calibrated online as new data comes in to avoid drift.

* **Interpretability vs. Complexity:** There is a tension between making models more complex to boost accuracy and keeping them interpretable. The hybrid models with many components (attention, CNN, LSTM, etc.) are hard to interpret end-to-end ‚Äì we might gain some local interpretations (e.g. what the attention is focusing on), but the overall model is a nested black-box. Conversely, simpler models (like a single ODE with a physics basis) are more interpretable but may not capture all nuances. The Kolmogorov‚ÄìArnold network, while more interpretable than a generic MLP, is still an active research concept, and extracting human-understandable insights from it is non-trivial. Thus, one limitation is that **domain experts may find it difficult to trust and adopt these models** without clear explanation of how they work. Bridging this gap requires more work on explainable AI techniques tailored to NODEs (for example, analyzing the learned ODE vector field, or mapping neural components to physical processes).

* **Robustness to Operational Variability:** Many current studies (including those reviewed) use data from batteries cycled under controlled conditions (constant current/constant voltage charging, standard temperature, etc.). In real-world operation, loads are dynamic and environments vary. It‚Äôs unclear how robust these NODE models are to, say, a sudden change in load profile or ambient temperature excursions. A model might need additional inputs (mission profiles, temperatures) to remain accurate when conditions deviate from training. Handling **irregular usage patterns** is challenging ‚Äì standard NODE frameworks assume a more or less smooth time evolution, but EV usage can be intermittent and unpredictable. Some approaches like neural controlled differential equations (NCDEs) allow input-driven ODEs, which could be a solution, but have yet to be fully explored for batteries. In sum, adapting these models for the full complexity of field data (which includes rest periods, varying C-rates, etc.) is an ongoing challenge.

* **Lack of Standard Evaluation Protocols:** Finally, a practical limitation is the lack of standardized benchmarks for comparing these models. Each paper often introduces its own dataset splits, metrics, and even target definitions (some predict cycles to 80% capacity, others predict capacity itself). This makes direct comparison difficult and can obscure true progress. The field would benefit from common testbeds (much as the NASA dataset became a de facto benchmark for earlier works) for the new generation of models ‚Äì possibly including cross-dataset evaluation as a requirement. Without this, models might be overly tuned to specific data idiosyncrasies, limiting their real-world applicability.

## Future Directions

Research into NODE-based battery prognostics is rapidly evolving. Looking forward, several **future directions** are promising for addressing current limitations and pushing the performance further:

* **Toward Universal Battery Models:** A clear goal is to develop models that can handle *any* battery type, chemistry, or usage history with minimal retraining. This could be achieved via **transfer learning** or **meta-learning** approaches, where a model is pre-trained on a wide range of batteries (maybe using a Neural ODE backbone) and can be quickly adapted to a new battery with a small amount of data. Creating large and diverse training datasets (e.g. crowdsourced from EV fleets or public repositories) will be key. Physics-informed NODEs and UDEs will likely play a role here ‚Äì by embedding known universal physical laws (like diffusion, SEI growth kinetics, etc.), the model starts with a baseline that applies to any lithium-ion cell, then fine-tunes to specifics. Such universal models would drastically reduce the effort needed to deploy prognostics for new battery designs or applications.

* **Incorporation of Physical Constraints and Knowledge:** Future models will increasingly enforce **physical constraints** to ensure realistic predictions. This includes simple constraints (monotonic capacity decline, non-negativity of health indices) as well as complex ones (e.g. mass balance, energy conservation, temperature effects). Techniques like penalizing violations in the loss function or hard-coding constraint satisfaction (for example, using a sigmoid to bound SOH between 0 and 1\) will become standard. We may also see **hybrid modeling** where a part of the model is a known equation ‚Äì for instance, using an equivalent circuit model or a single-particle model to generate a physics-based feature, with a Neural ODE modeling the discrepancy. Physics-informed neural networks (PINNs) have started this trend[\[64\]](https://arxiv.org/html/2410.14347v1#:~:text=Wang%20et%20al.%20,with%20Neural%20ODEs%20or%20UDEs)[\[65\]](https://arxiv.org/html/2410.14347v1#:~:text=Ye%20et%20al.%20,world%20validation%20remains%20a%20limitation), and we expect more synergy between electrochemical modeling and machine learning. By blending first-principles with data-driven ODE learning, models can achieve better **extrapolation** (e.g. handling changes in temperature or load that were not in the training data, because the physics part knows the effect qualitatively).

* **Advanced Architectures (Transformers, Graph Neural ODEs):** On the algorithmic side, we anticipate more use of **transformers** and **graph neural networks** within the NODE context. A transformer-based Neural ODE could, for example, use self-attention to attend to important cycles in the past (like those where a sudden jump in resistance occurred) while the ODE models the gradual trend. Similarly, Graph Neural ODEs could model interactions in battery packs or incorporate spatially distributed data (like thermal gradients across a cell, or multi-electrode systems). The Int. J. Fatigue 2023 work combining GAT and transformers for RUL[\[68\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=,Int%20J%20Fatigue%202023%3B174%3A107722)[\[69\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=via%20a%20deep%20adaptive%20transformer,Int%20J%20Fatigue%202023%3B174%3A107722) suggests that *attention-enhanced temporal modeling* is a fruitful area. Continuous-depth versions of such networks (transformer-ODE hybrids) might offer the ability to capture both long-term dependencies and continuous dynamics. The challenge will be keeping these models tractable, but research in other domains (e.g. continuous transformers) could translate to batteries.

* **Uncertainty Quantification and Reliability:** As these models move toward deployment, quantifying uncertainty will be crucial. Future research should integrate methods like **Bayesian Neural ODEs** or Monte Carlo dropout to give confidence intervals for SOH/RUL predictions. Knowing the uncertainty can enable more robust decision-making (e.g. if the model is unsure, the BMS may act conservatively). There is also interest in **probabilistic RUL prediction**, giving a distribution of remaining life rather than a point estimate. Techniques from prognostics (like particle filters or Bayesian filters) could be combined with Neural ODE outputs to update distributions in real time. Some initial steps in Bayesian NODEs exist (for example, treating the ODE parameters as distributions), and applying them to battery health will enhance reliability.

* **Online Learning and Adaptation:** Batteries age in idiosyncratic ways depending on how they are used. A model that can **update itself online** as new data comes in would be extremely useful. Research might explore online training of Neural ODEs (which is tricky but feasible with careful design) or using approaches like federated learning for battery fleets. An exciting direction is to have a base model that personalizes to each battery ‚Äì for example, using a few early cycles of a new battery to calibrate a pretrained ODE model. This could be achieved by only updating a small subset of model parameters (like a bias term representing per-battery characteristics) online, which would be faster than full retraining. Continual learning techniques would help the model not forget prior knowledge while learning new patterns as the battery enters different life phases.

* **Benchmarking and Standardization:** Finally, the community would benefit from standard benchmarks and open-source implementations of these advanced models. We expect the creation of benchmark tasks (e.g. predict RUL given first 100 cycles, across multiple datasets) that all new models must report on. This will drive fair comparisons and faster iteration. Collaboration with industry to obtain real-world cycling data (including fast charging, partial cycling, etc.) for testing these models is another direction ‚Äì ensuring that academic advances actually translate to practical improvements on real EV or grid batteries.

In conclusion, Neural ODE-based models have quickly become a **frontier technology** in battery SOH and RUL prediction. By marrying deep learning with the mathematical elegance of differential equations, they open up new possibilities for accuracy and insight in modeling battery degradation. Ongoing research is making these models more general, interpretable, and usable in practice. The ultimate vision is a robust, physics-aware Neural ODE model that can serve as a ‚Äúdigital twin‚Äù for any battery cell ‚Äì continuously estimating its health, predicting its future, and doing so in a way that engineers can trust and understand. The progress from 2023 to 2025, as surveyed here, shows substantial strides toward this vision, and future innovations will likely bring us even closer to reliable, universal battery prognostics.

**References:** (Provided inline as per the citations in text)

---

[\[1\]](file://file_000000009f6c6246adbcf605606da613#:~:text=with%20neural%20ODEs%2C%20augmented%20neural,recurrent%20ODEs%20are%20compared%2021) [\[2\]](file://file_000000009f6c6246adbcf605606da613#:~:text=ANODEs%20have%20been%20introduced%20recently,ODEs.%20In%20161) [\[3\]](file://file_000000009f6c6246adbcf605606da613#:~:text=augmented%20neural%20ODE%20%28ANODE%29%20,shown%20to%20be%20more%2081) [\[4\]](file://file_000000009f6c6246adbcf605606da613#:~:text=robust%20and%20lightweight%20and%20better,ODE%20%5B52%5D.%2082) [\[5\]](file://file_000000009f6c6246adbcf605606da613#:~:text=have%20shown%20that%20charge%20voltage,were%20also%20included%20in%20105) [\[6\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%92%9A%F0%9D%91%98%20%3D%20) [\[7\]](file://file_000000009f6c6246adbcf605606da613#:~:text=87) [\[8\]](file://file_000000009f6c6246adbcf605606da613#:~:text=retrieving%20underlying%20dynamics%20and%20make,RNN%20can%2088) [\[9\]](file://file_000000009f6c6246adbcf605606da613#:~:text=achieves%20the%20best%20performance%2C%20except,an%20average%20RMSEsSOH%20lower%20226) [\[10\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%F0%9D%91%81TP%2FEOL%20%3D70,256) [\[11\]](file://file_000000009f6c6246adbcf605606da613#:~:text=At%20%F0%9D%91%81TP%2FEOL%20%3D60,trend%20typical%20of%20battery%20259) [\[12\]](file://file_000000009f6c6246adbcf605606da613#:~:text=In%20neural,Dopri5%2C%20190) [\[13\]](file://file_000000009f6c6246adbcf605606da613#:~:text=dropout%20%28p%3D0,the%20model%20architectures%20is%20194) [\[14\]](file://file_000000009f6c6246adbcf605606da613#:~:text=The%20,RNN%29%2C%2083) [\[15\]](file://file_000000009f6c6246adbcf605606da613#:~:text=differencing%20is%20used%20to%20obtain,from%20%F0%9D%92%9A%F0%9D%91%98%2C%20namely%2C%20%20171) [\[16\]](file://file_000000009f6c6246adbcf605606da613#:~:text=obtained%20using%20173) [\[17\]](file://file_000000009f6c6246adbcf605606da613#:~:text=RNNs%20are%20attractive%20given%20their,53%2C%20168) [\[18\]](file://file_000000009f6c6246adbcf605606da613#:~:text=%28ANODE%29%2C%203.27%25%20%28PC,The%20larger%20errors%20can%20be) [\[19\]](file://file_000000009f6c6246adbcf605606da613#:~:text=augmented%20200) [\[20\]](file://file_000000009f6c6246adbcf605606da613#:~:text=space%20dimensions%20,RNN%20underlying%20201) neural-ordinary-differential-equations-and-recurrent-neural-networks-for-predicting-the-state-of-health-of-batteries.pdf

[file://file\_000000009f6c6246adbcf605606da613](file://file_000000009f6c6246adbcf605606da613)

[\[21\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,state%20trajectories%20during%20the%20degradation) [\[22\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=layers%20in%20FC%20,proposed%20to%20replace%20fixed%20activation) [\[23\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Fig,external%20indicators%20at%20different%20moments) [\[24\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=relations%20are%20introduced%20by%20using,supplement%20the%20interdependencies%20between%20physical) [\[25\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=fully%20connected%20neural%20network%2C%20KAN,the%20binary%20operation%20of%20addition) [\[26\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=These%20matrices%20illustrate%20a%20Kolmogorov%E2%80%93Arnold,5%2C%20which%20is%20constructed%20as) [\[27\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=functions%20inspired%20by%20the%20Kolmogorov%E2%80%93Arnold,connected%20neural) [\[28\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=approximated%20by%20Kolmogorov%E2%80%93Arnold%20network%20,dynamic%20degradation%20information%20of%20SSLIBs) [\[29\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=obtaining%20comprehensive%20implicit%20information%20during,KAN) [\[30\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Comparison%20results%20of%20different%20methods,0726) [\[31\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Method%20%F0%9D%91%852%20MAE%20SMAPE%20RMSE,9132) [\[32\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Table%203%20Comparison%20results%20of,5371) [\[33\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=optimal%20minimum%20root%20mean%20square,provide%20new%20perspectives%20and%20solutions) [\[34\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=ordinary%20differential%20equation%20,in%20the%20real%20SSLIBs%20degradation) [\[61\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=work%20,batteries%20to%20improve%20the%20computational) [\[62\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=Method%20%F0%9D%91%852%20MAE%20SMAPE%20RMSE,9132) [\[68\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=,Int%20J%20Fatigue%202023%3B174%3A107722) [\[69\]](file://file_00000000c044620a9e7c2c1399d6896a#:~:text=via%20a%20deep%20adaptive%20transformer,Int%20J%20Fatigue%202023%3B174%3A107722) Remaining useful life prediction for solid-state lithium batteries based on spatial‚Äìtemporal relations and neuronal ODE-assisted KAN.pdf

[file://file\_00000000c044620a9e7c2c1399d6896a](file://file_00000000c044620a9e7c2c1399d6896a)

[\[35\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=We%20construct%20a%20hybrid%20model,which%20integrates%20the%20attention%20mechanism) [\[36\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3) [\[37\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=Fig,the%20model%20architecture) [\[38\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3) [\[39\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model) [\[40\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=CNNs%20are%20commonly%20used%20for,They) [\[41\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=connected%20layer%20,data%20to%20the%20ANODE%20solver) [\[42\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=match%20at%20L268%20on%20NODE,The%20proposed%20model%20integrates) [\[43\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=on%20NODE%20in%20combination%20with,The%20proposed%20model%20integrates) [\[44\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=model%20employs%20normalized%20charging%20time,to%20specific%20voltages%20in%20the) [\[45\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=,Specifically%2C%20the%20input%20feature) [\[46\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=remaining) [\[47\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=useful%20of%20life,datasets%20and%20validated%20on%20the) [\[48\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=adeptly%20applied%20to%20the%20prediction,RUL%2C%20achieving%20high%20accuracy%20results) [\[49\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=useful%20of%20life,datasets%20and%20validated%20on%20the) [\[50\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=ordinary%20differential%20equation%20%28ODE%29,ODE%20%28NODE%29%20%5B35%5D%20and) [\[51\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=complex%20temporal%20dynamics,the%20model%E2%80%99s%20performance%20with%20its) [\[52\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3.2%20Implementation%20and%20hyper) [\[53\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=The%20ANODE%20model%20uses%20an,20%20and%20applies%20the%20default) [\[63\]](file://file_00000000a8ec624684fc90aaaa381bc6#:~:text=3) 2505.05803.pdf

[file://file\_00000000a8ec624684fc90aaaa381bc6](file://file_00000000a8ec624684fc90aaaa381bc6)

[\[54\]](https://arxiv.org/html/2410.14347v1#:~:text=to%20the%20prediction%20and%20long,that%20both%20the%20predictions%20and) [\[55\]](https://arxiv.org/html/2410.14347v1#:~:text=forecasts%20reflect%20practical%20conditions,This%20integration%20of%20data) [\[56\]](https://arxiv.org/html/2410.14347v1#:~:text=Differential%20Equations%20,achieved%20a%20mean%20squared%20error) [\[57\]](https://arxiv.org/html/2410.14347v1#:~:text=systems%2C%20SciML%20integrates%20domain%20knowledge,49%20in%20the%20NeuralODE) [\[58\]](https://arxiv.org/html/2410.14347v1#:~:text=term%20battery%20health%20and%20forecasting,strengths%20in%20interpretability%20and%20scalability) [\[59\]](https://arxiv.org/html/2410.14347v1#:~:text=predictive%20accuracy%20while%20reducing%20data,to%20the%20sustainability%20of%20energy) [\[64\]](https://arxiv.org/html/2410.14347v1#:~:text=Wang%20et%20al.%20,with%20Neural%20ODEs%20or%20UDEs) [\[65\]](https://arxiv.org/html/2410.14347v1#:~:text=Ye%20et%20al.%20,world%20validation%20remains%20a%20limitation) A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles

[https://arxiv.org/html/2410.14347v1](https://arxiv.org/html/2410.14347v1)

[\[60\]](https://www.researchgate.net/publication/395488447_Joint_prediction_of_SOH_and_RUL_for_Lithium-ion_batteries_by_an_enhanced_Transformer_model_with_physical_information_constraints#:~:text=Joint%20prediction%20of%20SOH%20and,model%20with%20physical%20information%20constraints) [\[66\]](https://www.researchgate.net/publication/395488447_Joint_prediction_of_SOH_and_RUL_for_Lithium-ion_batteries_by_an_enhanced_Transformer_model_with_physical_information_constraints#:~:text=demanding%20conditions,techniques%20and%20the%20machine%20learning) [\[67\]](https://www.researchgate.net/publication/395488447_Joint_prediction_of_SOH_and_RUL_for_Lithium-ion_batteries_by_an_enhanced_Transformer_model_with_physical_information_constraints#:~:text=study%20discusses%20recent%20approaches%20to,the%20standard%20in%20SOH%20prediction) Joint prediction of SOH and RUL for Lithium-ion batteries by an enhanced Transformer model with physical information constraints

[https://www.researchgate.net/publication/395488447\_Joint\_prediction\_of\_SOH\_and\_RUL\_for\_Lithium-ion\_batteries\_by\_an\_enhanced\_Transformer\_model\_with\_physical\_information\_constraints](https://www.researchgate.net/publication/395488447_Joint_prediction_of_SOH_and_RUL_for_Lithium-ion_batteries_by_an_enhanced_Transformer_model_with_physical_information_constraints)